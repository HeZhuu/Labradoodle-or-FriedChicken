##Set values for the parameters:
if (Option_1){
candidates<-list(cost=c(.1,0.5,1,1.5,2,3),gamma=c(.001,0.005,0.08,.01,0.5))
#candidates<-list(cost=c(.1,0.5,1,1.5,3,5,7),gamma=c(0.5,1,5,10,20,30))
} else {
candidates<-list(cost=c(.1,0.3,0.5,0.7,0.9,1.1,1.3),gamma=c(.001,0.003,0.005,0.007,.01,0.03))
}
##Tune the model:
tc<-tune.control(cross = K)  #set K-folds based on user-selection
svm_tune <- tune(svm, train.x=Train.x, train.y=Train.y,
kernel="radial",scale=F,ranges=candidates,tunecontrol = tc)
##Get the performacne for each model:
performace_svm<-svm_tune$svm_tune
##Save resilts:
save(performace_svm, file="../output/performance_cv.RData")
}
if (run.cv){
##Plot 3D version:
plot(svm_tune ,type='persp', main ="CV error vs different value of margin & bandwidth parameter")
##Plot 2D version(cost as x-axis):
performace_svm$gamma2<-as.factor(performace_svm$gamma)
ggplot(data=performace_svm)+ geom_line(aes(x=cost,y=error,linetype=gamma2,col=gamma2))
##Plot 2D version(gamma as x-axis):
performace_svm$cost2<-as.factor(performace_svm$cost)
ggplot(data=performace_svm)+ geom_line(aes(x=gamma,y=error,linetype=cost2,col=cost2))
}
performace_svm
performace_svm<-svm_tune$performances
save(performace_svm, file="../output/performance_cv.RData")
View(performace_svm)
if (run.cv){
##Plot 3D version:
plot(svm_tune ,type='persp', main ="CV error vs different value of margin & bandwidth parameter")
##Plot 2D version(cost as x-axis):
performace_svm$gamma2<-as.factor(performace_svm$gamma)
ggplot(data=performace_svm)+ geom_line(aes(x=cost,y=error,linetype=gamma2,col=gamma2))
##Plot 2D version(gamma as x-axis):
performace_svm$cost2<-as.factor(performace_svm$cost)
ggplot(data=performace_svm)+ geom_line(aes(x=gamma,y=error,linetype=cost2,col=cost2))
}
tm_train=NA
if (run.cv){
##Get the parameters with the best results:
best_cost<-performace_svm$cost[which.min(performace_svm$error)]
best_gamma<-performace_svm$gamma[which.min(performace_svm$error)]
##Train the model:
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,
gamma=best_gamma,cost = best_cost))
} else {
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y))
}
##Save the model:
save(fit_train, file="../output/fit_train.RData")
tm_test=NA
tm_test <- system.time(pred_newtest <- predict(fit_train, X_inde_test))
y22<- read.table("../data/extra_data/labels.csv", header=T)
y22<-as.factor(t(y22))
mean(pred_test==y22)
pred_test
mean(pred_test==y22)
tm_train<-svm(Train.x,Train.y,scale=F,kernel="linear",cost =1.2)
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
pre_test<-predict(fit_train,Test.x)
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
tm_train<-svm(Train.x,Train.y,scale=F,kernel="linear",cost =0.2)
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
tm_train
tm_train<-svm(Train.x,Train.y,scale=F,kernel="linear",cost =0.01)
if(set_seed){
set.seed(0)
Index<-sample(n,round(train_proportion*n,1),replace = F)
} else{
Index<-sample(n,round(train_proportion*n,1),replace = F)
}
#n is the No. of all provided data
Train.x<- data.matrix(X[Index,])
Train.y<-y[Index]
Test.x<-data.matrix(X[-Index,])
Test.y<-y[-Index]
if (run.cv){
##Set values for the parameters:
if (Option_1){
candidates<-list(cost=c(.1,0.5,1,1.5,2,3),gamma=c(.001,0.005,0.08,.01,0.5))
#candidates<-list(cost=c(.1,0.5,1,1.5,3,5,7),gamma=c(0.5,1,5,10,20,30))
} else {
candidates<-list(cost=c(.1,0.3,0.5,0.7,0.9,1.1,1.3),gamma=c(.001,0.003,0.005,0.007,.01,0.03))
}
##Tune the model:
tc<-tune.control(cross = K)  #set K-folds based on user-selection
svm_tune <- tune(svm, train.x=Train.x, train.y=Train.y,
kernel="radial",scale=F,ranges=candidates,tunecontrol = tc)
##Get the performacne for each model:
performace_svm<-svm_tune$performances
##Save resilts:
save(performace_svm, file="../output/performance_cv.RData")
}
tm_train<-svm(Train.x,Train.y,scale=F,kernel="linear",cost =0.01)
tm_train<-svm(Train.x,Train.y,scale=F,kernel="linear",cost =0.01)
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
tm_test <- system.time(pred_newtest <- predict(fit_train, X_inde_test))
mean(pred_test==y22)
dat_test<-load(file = ("../output/Newfeature_inde_test.RData"))
tm_test <- system.time(pred_newtest <- predict(fit_train,dat_test))
dat_test
load(file = ("../output/Newfeature_inde_test.RData"))
tm_test=NA
if(run.test){
load(file = ("../output/Newfeature_inde_test.RData"))
tm_test <- system.time(pred_newtest <- predict(fit_train,X_inde_test))
save(pred_newtest, file="../output/pred_newtest.RData")
}
#y2<- read.table("../data/extra_data/labels.csv", header=T)
#y2<-as.factor(t(y22))
#mean(pred_test==y22)
#pred_test
knitr::opts_chunk$set(echo = TRUE)
seq(0.005:0.9,sep=0.1)
seq(0.005:0.9,by=0.1)
cost=seq(0.005:0.9,by=0.1)
seq(0.005:0.9,by=0.1)
seq(0.005:0.9,by=0.01)
packages.used=c("e1071","EBImage","ggplot2")
packages.needed=setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
library(EBImage)
library(e1071)
library(ggplot2)
##here is where all the given 2000 pictures are put
experiment_dir <- "../data/training_data/raw_images/"
##here is where all the independent testing set are put (it will be given in class)
testing_dir <- "../data/extra_data/raw_images/"
set_seed=FALSE         #use set.seed() whenever randomization needed
run.cv=FALSE            # run cross-validation on the training set
K <- 5                 # number of CV folds
train_proportion=0.75  # Porportion of the data that used for training the model
new.feature.train =TRUE      #process features for gievn training set
new.feature.test=TRUE       # process features for independent testing set
run.test=TRUE        # run evaluation on an independent test set
y<- read.table("../data/training_data/labels.csv", header=T)
y<-as.factor(t(y))
n<-length(y)
source("../lib/extract_feature.R")
tm_feature_train <- NA
if(new.feature.train){
tm_feature_train<- system.time(X <- extract_feature(img_dir=experiment_dir,                                                 data_name="Newfeature_train", export=T))
#X is 2000*1024
} else {
X <-t(read.csv("../data/training_data/sift_features/sift_features.csv",header=T))
#X is 2000*5000
}
tm_feature_indetest<-NA
if(new.feature.test){
tm_feature_indetest<- system.time(X_inde_test<- extract_feature(testing_dir,                                                 data_name="Newfeature_inde_test", export=T))
save(X_inde_test,file="../output/Newfeature_inde_test.RData")
}
if(set_seed){
set.seed(0)
Index<-sample(n,round(train_proportion*n,1),replace = F)
} else{
Index<-sample(n,round(train_proportion*n,1),replace = F)
}
#n is the No. of all provided data
Train.x<- data.matrix(X[Index,])
Train.y<-y[Index]
Test.x<-data.matrix(X[-Index,])
Test.y<-y[-Index]
source("../lib/Train_lsvm.R")
Option_1<-TRUE  ##If you choose FALSE,Option 2 will be used
if (run.cv){
##Set values for the parameters:
if (Option_1){
candidates<-list(cost=seq(0.005:0.8,by=0.1))
} else {
candidates<-list(cost=seq(0.005:0.9,by=0.01))
}
##Tune the model:
tc<-tune.control(cross = K)  #set K-folds based on user-selection
svm_tune <- tune(svm, train.x=Train.x, train.y=Train.y,
kernel="linear",scale=F,ranges=candidates,tunecontrol = tc)
##Get the performacne for each model:
performace_svm<-svm_tune$performances
##Save resilts:
save(performace_svm, file="../output/performance_cv.RData")
}
if (run.cv){
plot(candidates,performace_svm$error,main = "CV error vs different value of cost",xlab = "Margin Parameter", ylab = "Estimated Accuracy",type="b")
}
tm_train=NA
if (run.cv){
##Get the parameters with the best results:
best_cost<-performace_svm$cost[which.min(performace_svm$error)]
##Train the model:
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,cost = best_cost))
} else {
tm_train<-svm(Train.x,Train.y)
}
##Save the model:
save(fit_train, file="../output/fit_train.RData")
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
tm_test=NA
if(run.test){
load(file = ("../output/Newfeature_inde_test.RData"))
load(file="../output/fit_train.RData")
tm_test <- system.time(pred_newtest <- predict(fit_train, X_inde_test))
save(pred_newtest, file="../output/pred_newtest.RData")
}
y22<- read.table("../data/extra_data/labels.csv", header=T)
y22<-as.factor(t(y22))
mean(pred_test==y22)
pred_test
new.feature.test
tm_feature_indetest<-NA
if(new.feature.test){
tm_feature_indetest<- system.time(X_inde_test<- extract_feature(testing_dir,                                                 data_name="Newfeature_inde_test", export=T))
save(X_inde_test,file="../output/Newfeature_inde_test.RData")
}
write.csv(X_inde_test,file="../output/Newfeature_inde_test.cvs")
write.csv(X_inde_test,file="../output/Newfeature_inde_test.csv")
dim(X_inde_test)
tm_train=NA
if (run.cv){
##Get the parameters with the best results:
best_cost<-performace_svm$cost[which.min(performace_svm$error)]
##Train the model:
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,cost = best_cost))
} else {
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y))
}
##Save the model:
save(fit_train, file="../output/fit_train.RData")
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
tm_test=NA
if(run.test){
load(file = ("../output/Newfeature_inde_test.RData"))
load(file="../output/fit_train.RData")
tm_test <- system.time(pred_newtest <- predict(fit_train, X_inde_test))
save(pred_newtest, file="../output/pred_newtest.RData")
}
y22<- read.table("../data/extra_data/labels.csv", header=T)
y22<-as.factor(t(y22))
mean(pred_test==y22)
if (run.test & new.feature.test){
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
} else{
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for making prediction=", tm_test[1], "s \n")
}
set_seed=FALSE         #use set.seed() whenever randomization needed
run.cv=FALSE            # run cross-validation on the training set
K <- 5                 # number of CV folds
train_proportion=0.75  # Porportion of the data that used for training the model
new.feature.train =FALSE      #process features for gievn training set
new.feature.test=FALSE      # process features for independent testing set
run.test=FALSE      # run evaluation on an independent test set
y<- read.table("../data/training_data/labels.csv", header=T)
y<-as.factor(t(y))
n<-length(y)
source("../lib/extract_feature.R")
tm_feature_train <- NA
if(new.feature.train){
tm_feature_train<- system.time(X <- extract_feature(img_dir=experiment_dir,                                                 data_name="Newfeature_train", export=T))
#X is 2000*1024
} else {
X <-t(read.csv("../data/training_data/sift_features/sift_features.csv",header=T))
#X is 2000*5000
}
tm_feature_indetest<-NA
if(new.feature.test){
tm_feature_indetest<- system.time(X_inde_test<- extract_feature(testing_dir,                                                 data_name="Newfeature_inde_test", export=T))
}
if(set_seed){
set.seed(0)
Index<-sample(n,round(train_proportion*n,1),replace = F)
} else{
Index<-sample(n,round(train_proportion*n,1),replace = F)
}
#n is the No. of all provided data
Train.x<- data.matrix(X[Index,])
Train.y<-y[Index]
Test.x<-data.matrix(X[-Index,])
Test.y<-y[-Index]
source("../lib/Train_lsvm.R")
Option_1<-TRUE  ##If you choose FALSE,Option 2 will be used
if (run.cv){
##Set values for the parameters:
if (Option_1){
candidates<-list(cost=seq(0.005:0.8,by=0.1))
} else {
candidates<-list(cost=seq(0.005:0.9,by=0.01))
}
##Tune the model:
tc<-tune.control(cross = K)  #set K-folds based on user-selection
svm_tune <- tune(svm, train.x=Train.x, train.y=Train.y,
kernel="linear",scale=F,ranges=candidates,tunecontrol = tc)
##Get the performacne for each model:
performace_svm<-svm_tune$performances
##Save resilts:
save(performace_svm, file="../output/performance_cv.RData")
}
if (run.cv){
plot(candidates,performace_svm$error,main = "CV error vs different value of cost",xlab = "Margin Parameter", ylab = "Estimated Accuracy",type="b")
}
tm_train=NA
if (run.cv){
##Get the parameters with the best results:
best_cost<-performace_svm$cost[which.min(performace_svm$error)]
##Train the model:
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,cost = best_cost))
} else {
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y))
}
##Save the model:
save(fit_train, file="../output/fit_train.RData")
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,cost=2))
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,cost=60))
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,cost=80))
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,cost=85))
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
set_seed=FALSE         #use set.seed() whenever randomization needed
run.cv=T           # run cross-validation on the training set
K <- 5                 # number of CV folds
train_proportion=0.75  # Porportion of the data that used for training the model
new.feature.train =T     #process features for gievn training set
new.feature.test=T      # process features for independent testing set
run.test=T     # run evaluation on an independent test set
y<- read.table("../data/training_data/labels.csv", header=T)
y<-as.factor(t(y))
n<-length(y)
source("../lib/extract_feature.R")
tm_feature_train <- NA
if(new.feature.train){
tm_feature_train<- system.time(X <- extract_feature(img_dir=experiment_dir,                                                 data_name="Newfeature_train", export=T))
#X is 2000*1024
} else {
X <-t(read.csv("../data/training_data/sift_features/sift_features.csv",header=T))
#X is 2000*5000
}
tm_feature_indetest<-NA
if(new.feature.test){
tm_feature_indetest<- system.time(X_inde_test<- extract_feature(testing_dir,                                                 data_name="Newfeature_inde_test", export=T))
}
if(set_seed){
set.seed(0)
Index<-sample(n,round(train_proportion*n,1),replace = F)
} else{
Index<-sample(n,round(train_proportion*n,1),replace = F)
}
#n is the No. of all provided data
Train.x<- data.matrix(X[Index,])
Train.y<-y[Index]
Test.x<-data.matrix(X[-Index,])
Test.y<-y[-Index]
source("../lib/Train_lsvm.R")
source("../lib/Train_lsvm.R")
Option_1<-TRUE  ##If you choose FALSE,Option 2 will be used
Option_1<-TRUE  ##If you choose FALSE,Option 2 will be used
if (run.cv){
plot(candidates,performace_svm$error,main = "CV error vs different value of cost",xlab = "Margin Parameter", ylab = "Estimated Accuracy",type="b")
}
if (run.cv){
##Set values for the parameters:
if (Option_1){
candidates<-list(cost=seq(0.005:0.8,by=0.1))
} else {
candidates<-list(cost=seq(0.005:0.9,by=0.01))
}
##Tune the model:
tc<-tune.control(cross = K)  #set K-folds based on user-selection
svm_tune <- tune(svm, train.x=Train.x, train.y=Train.y,
kernel="linear",scale=F,ranges=candidates,tunecontrol = tc)
##Get the performacne for each model:
performace_svm<-svm_tune$performances
##Save resilts:
save(performace_svm, file="../output/performance_cv.RData")
}
tm_train=NA
if (run.cv){
##Get the parameters with the best results:
best_cost<-performace_svm$cost[which.min(performace_svm$error)]
##Train the model:
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,cost = best_cost))
} else {
if (new.feature.train){
##For SIFT and our new features, we have different default cost:
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,cost=0.05))
} else {
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,cost=85))
}
}
##Save the model:
save(fit_train, file="../output/fit_train.RData")
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
tm_test=NA
if(run.test){
load(file = ("../output/Newfeature_inde_test.RData"))
load(file="../output/fit_train.RData")
tm_test <- system.time(pred_newtest <- predict(fit_train, X_inde_test))
save(pred_newtest, file="../output/pred_newtest.RData")
}
y22<- read.table("../data/extra_data/labels.csv", header=T)
y22<-as.factor(t(y22))
mean(pred_test==y22)
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,cost=0.005))
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,cost=0.001))
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,cost=0.003))
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
tm_test=NA
if(run.test){
load(file = ("../output/Newfeature_inde_test.RData"))
load(file="../output/fit_train.RData")
tm_test <- system.time(pred_newtest <- predict(fit_train, X_inde_test))
save(pred_newtest, file="../output/pred_newtest.RData")
}
y22<- read.table("../data/extra_data/labels.csv", header=T)
y22<-as.factor(t(y22))
mean(pred_test==y22)
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,cost=0.003))
tm_test=NA
if(run.test){
#load(file = ("../output/Newfeature_inde_test.RData"))
#load(file="../output/fit_train.RData")
tm_test <- system.time(pred_newtest <- predict(fit_train, X_inde_test))
save(pred_newtest, file="../output/pred_newtest.RData")
}
y22<- read.table("../data/extra_data/labels.csv", header=T)
y22<-as.factor(t(y22))
mean(pred_test==y22)
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,cost=0.2))
